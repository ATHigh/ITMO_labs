{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install --quiet transformers==4.37.2 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2 datasets==2.14.7","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:21:53.909618Z","iopub.execute_input":"2024-03-03T22:21:53.909981Z","iopub.status.idle":"2024-03-03T22:22:44.257542Z","shell.execute_reply.started":"2024-03-03T22:21:53.909951Z","shell.execute_reply":"2024-03-03T22:22:44.256448Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm.auto import tqdm, trange\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport peft\n\nimport transformers\nfrom datasets import load_dataset\n\nimport random\nconst_seed = 100","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:22:44.259565Z","iopub.execute_input":"2024-03-03T22:22:44.259868Z","iopub.status.idle":"2024-03-03T22:22:59.570135Z","shell.execute_reply.started":"2024-03-03T22:22:44.259839Z","shell.execute_reply":"2024-03-03T22:22:59.569407Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"assert torch.cuda.is_available(), \"check out cuda availability (change runtime type in colab)\"","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:22:59.571298Z","iopub.execute_input":"2024-03-03T22:22:59.571873Z","iopub.status.idle":"2024-03-03T22:22:59.576438Z","shell.execute_reply.started":"2024-03-03T22:22:59.571838Z","shell.execute_reply":"2024-03-03T22:22:59.575599Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:22:59.578804Z","iopub.execute_input":"2024-03-03T22:22:59.579484Z","iopub.status.idle":"2024-03-03T22:22:59.588141Z","shell.execute_reply.started":"2024-03-03T22:22:59.579457Z","shell.execute_reply":"2024-03-03T22:22:59.587427Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Part 0: Initializing the model and tokenizer","metadata":{}},{"cell_type":"markdown","source":"let's take mistral model for our experiments (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) that was tuned to follow user instructions. Pay attention that we load model in 4 bit to decrease the memory usage.","metadata":{}},{"cell_type":"markdown","source":"model_name = 'mistralai/Mistral-7B-Instruct-v0.2'","metadata":{}},{"cell_type":"code","source":"model_name = 'mistralai/Mistral-7B-Instruct-v0.2'","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:22:59.589198Z","iopub.execute_input":"2024-03-03T22:22:59.589828Z","iopub.status.idle":"2024-03-03T22:22:59.606348Z","shell.execute_reply.started":"2024-03-03T22:22:59.589796Z","shell.execute_reply":"2024-03-03T22:22:59.605471Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# load llama tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Note: to speed up inference you can use flash attention 2 (https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n    load_in_4bit=True, torch_dtype=torch.float32,  #attn_implementation=\"flash_attention_2\"  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:22:59.607610Z","iopub.execute_input":"2024-03-03T22:22:59.607974Z","iopub.status.idle":"2024-03-03T22:25:32.621776Z","shell.execute_reply.started":"2024-03-03T22:22:59.607902Z","shell.execute_reply":"2024-03-03T22:25:32.620823Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcfd646b079f4b5f87222283c7d52236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c1532d4e454d7081befac1a8c2d0fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"505882f975ab4a4392f6ca04243c8216"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49e42dd2328c4b358831a6920901280e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83b52b49d2c04b568bb3e342072f81ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3448924d86d04c20ba67828ee2d7ef7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"995c4d5a18314e479c74aed9622b8ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1193ae5eda924b539cf03e932a6765dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55a77fae57b40cb8d1652424a6b1c50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6dd9b7bb3a24e6c97e34d5b3798062e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64b33b0a5ac2492587a98af0401f70e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bec17d8236f941a691d8cbeaed771841"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Part 1 (5 points): Prompt-engineering ","metadata":{}},{"cell_type":"markdown","source":"**There are different strategies for text generation in huggingface:**\n\n| Strategy | Description | Pros & Cons |\n| --- | --- | --- |\n| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n\nDocumentation references:\n- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)","metadata":{}},{"cell_type":"code","source":"# TODO: create a function for generation with huggingface\ndef get_answer(tokenizer, model, messages, max_new_tokens=200, \n               temperature=0.5, do_sample=True):\n    # TODO: tokenize input, generate answer and decode output. Pay attention to tokenizer methods\n    \n#     encodeds = tokenizer.apply_chat_template(conversation=messages,tokenize = True,\n#                                              padding=True, truncation=True,return_tensors=\"pt\")\n#     encodeds = tokenizer(messages[0]['content'], truncation=True, padding=True, return_tensors=\"pt\")\n\n    encodeds = tokenizer(messages, truncation=True, padding=True, return_tensors=\"pt\")\n    generated_ids = model.generate(**encodeds, max_new_tokens=max_new_tokens,\n                                   temperature=temperature, do_sample=do_sample)\n    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    \n    return decoded","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:25:32.624132Z","iopub.execute_input":"2024-03-03T22:25:32.624427Z","iopub.status.idle":"2024-03-03T22:25:32.630090Z","shell.execute_reply.started":"2024-03-03T22:25:32.624402Z","shell.execute_reply":"2024-03-03T22:25:32.629236Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"****I tried a lot of ways to increase the quality of the output, as for me it is strange, that with .apply_chat_template tokenizer the output is pretty bad. This is not commented code provide much more reliable answer****","metadata":{}},{"cell_type":"code","source":"# Let's try our model \n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Write an explanation of tensors for 5 year old\"},\n]\n\nprint(get_answer(tokenizer, model, messages[0]['content'])[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:36:37.953535Z","iopub.execute_input":"2024-03-03T21:36:37.954469Z","iopub.status.idle":"2024-03-03T21:36:48.590512Z","shell.execute_reply.started":"2024-03-03T21:36:37.954431Z","shell.execute_reply":"2024-03-03T21:36:48.589498Z"},"trusted":true},"execution_count":308,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1413: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Write an explanation of tensors for 5 year olds.\n\nTensors are special kinds of toys that can change shape in many different ways. Imagine you have a toy cube that can also be a long, flat rectangle or a round ball. But this toy is very smart, it can change shape in many different ways, not just a few. Tensors are like that toy, but for numbers and shapes. They can hold information in many different ways, just like our toy can be many different shapes. And just like how we can do things with our toy, like stacking or counting, we can do things with tensors, like calculating and solving problems.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You should obtain an explanation from the model. If so, let us go further!","metadata":{}},{"cell_type":"markdown","source":"Now we will take a sample from boolQ (https://huggingface.co/datasets/google/boolq) dataset and try prompting techniques to extract the needed answer and calculate its quality","metadata":{}},{"cell_type":"code","source":"df = load_dataset(\"google/boolq\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-03T22:25:32.631095Z","iopub.execute_input":"2024-03-03T22:25:32.631359Z","iopub.status.idle":"2024-03-03T22:25:35.612674Z","shell.execute_reply.started":"2024-03-03T22:25:32.631337Z","shell.execute_reply":"2024-03-03T22:25:35.611800Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd1c8c017fe34e609b684bac7117bab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25bbd3ad6a8d462496dd5efb93d93dd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e680c3db04543b7a17c263d4fd213e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e38ab54c1df4b0ca55d56197e6d0db6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"235a1905c5e440c8881053eead5a9c96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9427 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264885cd302848aaae6215ca1959e9a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3270 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738920989fb347d68afed98b04a04488"}},"metadata":{}}]},{"cell_type":"code","source":"# Fixing 20 validation examples\n\nrandom.seed(const_seed)\nidx = random.sample(range(1, 3270), 20)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:25:35.613939Z","iopub.execute_input":"2024-03-03T22:25:35.614215Z","iopub.status.idle":"2024-03-03T22:25:35.618564Z","shell.execute_reply.started":"2024-03-03T22:25:35.614191Z","shell.execute_reply":"2024-03-03T22:25:35.617722Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# sample you will work with\ndf_sample = df[\"validation\"].select(idx)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:25:35.619759Z","iopub.execute_input":"2024-03-03T22:25:35.620065Z","iopub.status.idle":"2024-03-03T22:25:35.650353Z","shell.execute_reply.started":"2024-03-03T22:25:35.620041Z","shell.execute_reply":"2024-03-03T22:25:35.649570Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# For instance, you can construct your prompt the following way\nmessages = [\n    {\"role\": \"user\", \"content\": '''You are given a text and question. Answer only \"true\" or \"false\".\ntext: As with other games in The Elder Scrolls series, the game is set on the continent of Tamriel. The events of the game occur a millennium before those of The Elder Scrolls V: Skyrim and around 800 years before The Elder Scrolls III: Morrowind and The Elder Scrolls IV: Oblivion. It has a broadly similar structure to Skyrim, with two separate conflicts progressing at the same time, one with the fate of the world in the balance, and one where the prize is supreme power on Tamriel. In The Elder Scrolls Online, the first struggle is against the Daedric Prince Molag Bal, who is attempting to meld the plane of Mundus with his realm of Coldharbour, and the second is to capture the vacant imperial throne, contested by three alliances of the mortal races. The player character has been sacrificed to Molag Bal, and Molag Bal has stolen their soul, the recovery of which is the primary game objective.\nquestion: is elder scrolls online the same as skyrim\nanswer: '''},\n]\n\nprint(get_answer(tokenizer, model, messages[0]['content'])[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:25:35.651433Z","iopub.execute_input":"2024-03-03T22:25:35.651698Z","iopub.status.idle":"2024-03-03T22:26:01.382572Z","shell.execute_reply.started":"2024-03-03T22:25:35.651676Z","shell.execute_reply":"2024-03-03T22:26:01.381518Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1413: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n2024-03-03 22:25:42.250881: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-03 22:25:42.251010: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-03 22:25:42.533336: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"You are given a text and question. Answer only \"true\" or \"false\".\ntext: As with other games in The Elder Scrolls series, the game is set on the continent of Tamriel. The events of the game occur a millennium before those of The Elder Scrolls V: Skyrim and around 800 years before The Elder Scrolls III: Morrowind and The Elder Scrolls IV: Oblivion. It has a broadly similar structure to Skyrim, with two separate conflicts progressing at the same time, one with the fate of the world in the balance, and one where the prize is supreme power on Tamriel. In The Elder Scrolls Online, the first struggle is against the Daedric Prince Molag Bal, who is attempting to meld the plane of Mundus with his realm of Coldharbour, and the second is to capture the vacant imperial throne, contested by three alliances of the mortal races. The player character has been sacrificed to Molag Bal, and Molag Bal has stolen their soul, the recovery of which is the primary game objective.\nquestion: is elder scrolls online the same as skyrim\nanswer: \nfalse. While there are similarities in the structure of the game and the setting, The Elder Scrolls Online is not the same game as Skyrim. They are different games set in the same universe, with different objectives, conflicts, and player characters.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Is anything wrong with the output? Now it is time for you to play around and try to come up with some better prompt.","metadata":{}},{"cell_type":"code","source":"from datasets import load_metric\nacc = load_metric('accuracy')","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:26:01.384306Z","iopub.execute_input":"2024-03-03T22:26:01.385175Z","iopub.status.idle":"2024-03-03T22:26:02.174779Z","shell.execute_reply.started":"2024-03-03T22:26:01.385137Z","shell.execute_reply":"2024-03-03T22:26:02.173839Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/823127602.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  acc = load_metric('accuracy')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30780a3f21f84ec8b424891dd3599e59"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:26:02.178151Z","iopub.execute_input":"2024-03-03T22:26:02.178509Z","iopub.status.idle":"2024-03-03T22:26:02.183393Z","shell.execute_reply.started":"2024-03-03T22:26:02.178481Z","shell.execute_reply":"2024-03-03T22:26:02.182507Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# TODO: create function to evaluate answers\n# Note: you can adapt function for different answer structures, \n# but you should be able to automatically extract the target \"true\" or \"false\" components\ndef evaluate_answers(true_answers, predictions):\n    ans = []\n    for i in range(len(true_answers)):\n        if predictions[i].split('your answer:')[1].split('\\n')[0].find('true') != -1 or predictions[i].split('your answer:')[1].split('\\n')[0].find('True') != -1:\n            ans.append(True)\n        else:\n            ans.append(False)\n    score = acc.compute(predictions=ans, references = true_answers)\n    return ans, score","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:26:02.184861Z","iopub.execute_input":"2024-03-03T22:26:02.185718Z","iopub.status.idle":"2024-03-03T22:26:02.196104Z","shell.execute_reply.started":"2024-03-03T22:26:02.185685Z","shell.execute_reply":"2024-03-03T22:26:02.195136Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(few_shot_answers[6].split('your answer:')[1].split('\\n')[0])#.find('true')","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:04:52.463331Z","iopub.execute_input":"2024-03-03T21:04:52.464235Z","iopub.status.idle":"2024-03-03T21:04:52.469186Z","shell.execute_reply.started":"2024-03-03T21:04:52.464186Z","shell.execute_reply":"2024-03-03T21:04:52.468183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(few_shot_answers[6])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:04:06.947198Z","iopub.execute_input":"2024-03-03T21:04:06.948026Z","iopub.status.idle":"2024-03-03T21:04:06.953006Z","shell.execute_reply.started":"2024-03-03T21:04:06.947992Z","shell.execute_reply":"2024-03-03T21:04:06.951813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TODO: Try and compare \"naive\" prompting (your best hand-crafted variant), few-shot prompting (https://www.promptingguide.ai/techniques/fewshot) and chain-of-thought prompting (step-be-step thinking - https://www.promptingguide.ai/techniques/cot).\n\nSave the generation results into separate csv files and do not forget to attach them to your homework.","metadata":{}},{"cell_type":"code","source":"def validating_prompts(tokenizer, model, df_sample, example, naive=False, few_shot=False, chain_of_thought=False):\n    model_answers = []\n    messages = []\n    \n    if naive == True:\n        for sample in df_sample:\n            messages.append('You are given question. Answer only \"true\" or \"false\". Dont explain the answer.' \\\n                            + '\\nquestion: ' + sample['question'] + '\\nyour answer:')\n            \n        decoded = get_answer(tokenizer, model, messages)\n        model_answers.append(decoded)\n            \n    elif few_shot == True:\n        for sample in df_sample:\n            messages.append('You are given 2 examples. Answer only \"true\" or \"false\", dont explain.' \\\n                    + '\\nquestion: ' + example[133]['question'] + '\\nanswer: '+ str(example[133]['answer']) + '\\n' \\\n                    + '\\nquestion: ' + example[1564]['question'] + '\\nanswer: ' + str(example[1564]['answer']) + '\\n' \\\n                    + '\\nquestion: ' + sample['question'] + '\\nyour answer:')\n            \n        decoded = get_answer(tokenizer, model, messages)\n        model_answers.append(decoded)\n            \n    elif chain_of_thought == True:\n        for sample in df_sample:\n            messages.append('You are given 2 examples of text and question. Answer only \"true\" or \"false\".' \\\n                    + '\\nquestion: ' + example[133]['question'] + '\\ntext: ' + example[133]['passage'] + '\\nanswer: '+ str(example[133]['answer']) \\\n                    + '\\nquestion: ' + example[1564]['question'] + '\\ntext: ' + example[1564]['passage'] + '\\nanswer: ' + str(example[1564]['answer']) \\\n                    + '\\nquestion: ' + sample['question'] + '\\ntext: ' + sample['passage'] + '\\nyour answer:')\n        \n        decoded = get_answer(tokenizer, model, messages)\n        model_answers.append(decoded) \n\n        \n    return model_answers[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:26:02.197221Z","iopub.execute_input":"2024-03-03T22:26:02.197493Z","iopub.status.idle":"2024-03-03T22:26:02.209729Z","shell.execute_reply.started":"2024-03-03T22:26:02.197470Z","shell.execute_reply":"2024-03-03T22:26:02.208835Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"def validating_prompts(tokenizer, model, df_sample, example, naive=False, few_shot=False, chain_of_thought=False):\n    model_answers = []\n    \n    if naive == True:\n        for sample in df_sample:\n            message = [\n                {\n                    'role': \"user\",\n                    'content': 'You are given question. Answer only \"true\" or \"false\". Dont explain the answer.' \\\n                    + '\\nquestion: ' + sample['question'] + '\\nyour answer:'\n  \n                }\n            ]\n            \n            decoded = get_answer(tokenizer, model, message)\n            model_answers.append(decoded[0])\n            \n    elif few_shot == True:\n        for sample in df_sample:\n            message = [\n                {\n                    'role': \"user\",\n                    'content': f'You are given 2 examples. Answer only \"true\" or \"false\", dont explain.' \\\n                    + '\\nquestion: ' + example[133]['question'] + '\\nanswer: '+ str(example[133]['answer']) + '\\n' \\\n                    + '\\nquestion: ' + example[1564]['question'] + '\\nanswer: ' + str(example[1564]['answer']) + '\\n' \\\n                    + '\\nquestion: ' + sample['question'] + '\\nyour answer:'\n  \n                }\n            ]\n            \n            decoded = get_answer(tokenizer, model, message)\n            model_answers.append(decoded[0])\n            \n    elif chain_of_thought == True:\n        for sample in df_sample:\n            message = [\n                {\n                    'role': \"user\",\n                    'content': 'You are given 2 examples of text and question. Answer only \"true\" or \"false\".' \\\n                    + '\\nquestion: ' + example[133]['question'] + '\\ntext: ' + example[133]['passage'] + '\\nanswer: '+ str(example[133]['answer']) \\\n                    + '\\nquestion: ' + example[1564]['question'] + '\\ntext: ' + example[1564]['passage'] + '\\nanswer: ' + str(example[1564]['answer']) \\\n                    + '\\nquestion: ' + sample['question'] + '\\ntext: ' + sample['passage'] + '\\nyour answer:'\n  \n                }\n            ]\n\n            decoded = get_answer(tokenizer, model, message)\n            model_answers.append(decoded[0]) \n\n        \n    return model_answers","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:24:01.490241Z","iopub.execute_input":"2024-03-03T19:24:01.490622Z","iopub.status.idle":"2024-03-03T19:24:01.502340Z","shell.execute_reply.started":"2024-03-03T19:24:01.490592Z","shell.execute_reply":"2024-03-03T19:24:01.501329Z"}}},{"cell_type":"code","source":"naive_answers = validating_prompts(tokenizer, model, df_sample, df, naive=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-03T20:12:32.079621Z","iopub.execute_input":"2024-03-03T20:12:32.080003Z","iopub.status.idle":"2024-03-03T20:14:18.354629Z","shell.execute_reply.started":"2024-03-03T20:12:32.079974Z","shell.execute_reply":"2024-03-03T20:14:18.353461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(naive_answers[8])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:12:15.146394Z","iopub.execute_input":"2024-03-03T20:12:15.146771Z","iopub.status.idle":"2024-03-03T20:12:15.151614Z","shell.execute_reply.started":"2024-03-03T20:12:15.146733Z","shell.execute_reply":"2024-03-03T20:12:15.150566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"few_shot_answers = validating_prompts(tokenizer, model, df_sample, df['validation'], few_shot=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-03T20:21:38.893903Z","iopub.execute_input":"2024-03-03T20:21:38.894300Z","iopub.status.idle":"2024-03-03T20:23:28.896059Z","shell.execute_reply.started":"2024-03-03T20:21:38.894269Z","shell.execute_reply":"2024-03-03T20:23:28.894856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(few_shot_answers[2])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:24:05.658899Z","iopub.execute_input":"2024-03-03T20:24:05.659284Z","iopub.status.idle":"2024-03-03T20:24:05.664411Z","shell.execute_reply.started":"2024-03-03T20:24:05.659253Z","shell.execute_reply":"2024-03-03T20:24:05.663474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chain_of_thoughts_answers = validating_prompts(tokenizer, model, df_sample, df['validation'], chain_of_thought=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-03T20:30:24.280955Z","iopub.execute_input":"2024-03-03T20:30:24.281811Z","iopub.status.idle":"2024-03-03T20:33:34.293042Z","shell.execute_reply.started":"2024-03-03T20:30:24.281778Z","shell.execute_reply":"2024-03-03T20:33:34.291984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chain_of_thoughts_answers[16])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:35:07.729463Z","iopub.execute_input":"2024-03-03T20:35:07.730203Z","iopub.status.idle":"2024-03-03T20:35:07.734981Z","shell.execute_reply.started":"2024-03-03T20:35:07.730171Z","shell.execute_reply":"2024-03-03T20:35:07.733952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naive_answers[4].split('/INST] ')[1].split('\\n')[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:33:45.124239Z","iopub.execute_input":"2024-03-03T19:33:45.124607Z","iopub.status.idle":"2024-03-03T19:33:45.131157Z","shell.execute_reply.started":"2024-03-03T19:33:45.124577Z","shell.execute_reply":"2024-03-03T19:33:45.130185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# naive_extracted = []\n# for i in range(len(naive_answers)):\n#     if naive_answers[i].find('INST] True') != -1:\n#         naive_extracted.append(True)\n#     else:\n#         naive_extracted.append(False)\n# #naive_extracted","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:34:04.625236Z","iopub.execute_input":"2024-03-03T19:34:04.625912Z","iopub.status.idle":"2024-03-03T19:34:04.631182Z","shell.execute_reply.started":"2024-03-03T19:34:04.625877Z","shell.execute_reply":"2024-03-03T19:34:04.630050Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naive_extracted = evaluate_answers(df_sample[\"answer\"], naive_answers)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:05:46.983639Z","iopub.execute_input":"2024-03-03T21:05:46.984040Z","iopub.status.idle":"2024-03-03T21:05:46.996484Z","shell.execute_reply.started":"2024-03-03T21:05:46.984011Z","shell.execute_reply":"2024-03-03T21:05:46.995617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naive_df = pd.DataFrame(\n    data = {\n        'Question': df_sample['question'],\n        'Model_answer': naive_answers,\n        'True_answers': df_sample['answer'],\n        'Predictions': naive_extracted[0]\n    }\n)\n\nnaive_df.to_csv('/kaggle/working/naive_answers.csv', index=False)\nnaive_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:05:49.490077Z","iopub.execute_input":"2024-03-03T21:05:49.490695Z","iopub.status.idle":"2024-03-03T21:05:49.507638Z","shell.execute_reply.started":"2024-03-03T21:05:49.490663Z","shell.execute_reply":"2024-03-03T21:05:49.506723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Accuracy for the naive prompting is {naive_extracted[1]}')","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:05:51.239596Z","iopub.execute_input":"2024-03-03T21:05:51.240605Z","iopub.status.idle":"2024-03-03T21:05:51.245085Z","shell.execute_reply.started":"2024-03-03T21:05:51.240564Z","shell.execute_reply":"2024-03-03T21:05:51.244072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# few_shot_extracted = []\n# for i in range(len(few_shot_answers)):\n#     if few_shot_answers[i].find('your answer: True') != -1:\n#         few_shot_extracted.append(True)\n#     else:\n#         few_shot_extracted.append(False)\n# #few_shot_extracted","metadata":{"execution":{"iopub.status.busy":"2024-03-03T19:50:13.619539Z","iopub.execute_input":"2024-03-03T19:50:13.619912Z","iopub.status.idle":"2024-03-03T19:50:13.625212Z","shell.execute_reply.started":"2024-03-03T19:50:13.619881Z","shell.execute_reply":"2024-03-03T19:50:13.624193Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"few_shot_extracted = evaluate_answers(df_sample[\"answer\"], few_shot_answers)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:05:54.946002Z","iopub.execute_input":"2024-03-03T21:05:54.946394Z","iopub.status.idle":"2024-03-03T21:05:54.957722Z","shell.execute_reply.started":"2024-03-03T21:05:54.946362Z","shell.execute_reply":"2024-03-03T21:05:54.956853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"few_shot_df = pd.DataFrame(\n    data = {\n        'Question': df_sample['question'],\n        'Model_answer': few_shot_answers,\n        'True_answers': df_sample['answer'],\n        'Predictions': few_shot_extracted[0]\n    }\n)\n\nfew_shot_df.to_csv('/kaggle/working/few_shot_answers.csv', index=False)\nfew_shot_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:05:56.573965Z","iopub.execute_input":"2024-03-03T21:05:56.574341Z","iopub.status.idle":"2024-03-03T21:05:56.591731Z","shell.execute_reply.started":"2024-03-03T21:05:56.574311Z","shell.execute_reply":"2024-03-03T21:05:56.590771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Accuracy for the few_shot prompting is {few_shot_extracted[1]}')","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:05:59.545650Z","iopub.execute_input":"2024-03-03T21:05:59.546634Z","iopub.status.idle":"2024-03-03T21:05:59.551966Z","shell.execute_reply.started":"2024-03-03T21:05:59.546588Z","shell.execute_reply":"2024-03-03T21:05:59.550928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chain_of_thoughts_answers[19])","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:01:46.351314Z","iopub.execute_input":"2024-03-03T21:01:46.352543Z","iopub.status.idle":"2024-03-03T21:01:46.357036Z","shell.execute_reply.started":"2024-03-03T21:01:46.352504Z","shell.execute_reply":"2024-03-03T21:01:46.355996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# chain_of_thoughts_extracted = []\n# for i in range(len(chain_of_thoughts_answers)):\n#     if chain_of_thoughts_answers[i].find('is \"True\"') != -1 or chain_of_thoughts_answers[i].find('is \"True') != -1 or \\\n#     chain_of_thoughts_answers[i].find('is \"true') != -1 or chain_of_thoughts_answers[i].find('is True') != -1 or\\\n#     chain_of_thoughts_answers[i].find('is true') != -1 or chain_of_thoughts_answers[i].find('Answer: Yes') != -1 or\\\n#     chain_of_thoughts_answers[i].find('INST] True') != -1 or chain_of_thoughts_answers[i].find('Answer: True') != -1:\n#         chain_of_thoughts_extracted.append(True)\n#     else:\n#         chain_of_thoughts_extracted.append(False)\n# # chain_of_thoughts_extracted","metadata":{"execution":{"iopub.status.busy":"2024-03-03T17:15:37.073064Z","iopub.execute_input":"2024-03-03T17:15:37.073361Z","iopub.status.idle":"2024-03-03T17:15:37.080512Z","shell.execute_reply.started":"2024-03-03T17:15:37.073339Z","shell.execute_reply":"2024-03-03T17:15:37.079667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chain_of_thoughts_extracted = evaluate_answers(df_sample[\"answer\"], chain_of_thoughts_answers)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:07:07.285281Z","iopub.execute_input":"2024-03-03T21:07:07.286052Z","iopub.status.idle":"2024-03-03T21:07:07.297869Z","shell.execute_reply.started":"2024-03-03T21:07:07.286021Z","shell.execute_reply":"2024-03-03T21:07:07.296968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chain_of_thoughts_df = pd.DataFrame(\n    data = {\n        'Question': df_sample['question'],\n        'Model_answer': chain_of_thoughts_answers,\n        'True_answers': df_sample['answer'],\n        'Predictions': chain_of_thoughts_extracted[0]\n    }\n)\n\nchain_of_thoughts_df.to_csv('/kaggle/working/chain_of_thoughts_answers.csv', index=False)\nchain_of_thoughts_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:07:12.897995Z","iopub.execute_input":"2024-03-03T21:07:12.898732Z","iopub.status.idle":"2024-03-03T21:07:12.915797Z","shell.execute_reply.started":"2024-03-03T21:07:12.898700Z","shell.execute_reply":"2024-03-03T21:07:12.914776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Accuracy for the chain of thoughts prompting is {chain_of_thoughts_extracted[1]}')","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:07:10.615041Z","iopub.execute_input":"2024-03-03T21:07:10.615402Z","iopub.status.idle":"2024-03-03T21:07:10.620223Z","shell.execute_reply.started":"2024-03-03T21:07:10.615376Z","shell.execute_reply":"2024-03-03T21:07:10.619211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2 (5 points): Fine-tuning with PEFT and LoRA","metadata":{}},{"cell_type":"code","source":"%pip install trl -q","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:26:02.210836Z","iopub.execute_input":"2024-03-03T22:26:02.211089Z","iopub.status.idle":"2024-03-03T22:26:16.254962Z","shell.execute_reply.started":"2024-03-03T22:26:02.211067Z","shell.execute_reply":"2024-03-03T22:26:16.253825Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom peft import LoraConfig","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:26:16.256835Z","iopub.execute_input":"2024-03-03T22:26:16.257673Z","iopub.status.idle":"2024-03-03T22:26:17.926353Z","shell.execute_reply.started":"2024-03-03T22:26:16.257628Z","shell.execute_reply":"2024-03-03T22:26:17.925433Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM,\n                                      num_virtual_tokens=16,\n                                      prompt_tuning_init_text='Answer only \"true\" or \"false\"') #\nlora_config = LoraConfig(\n    r = 16,\n    lora_alpha = 10,\n    lora_dropout = 0.05,\n    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type = peft.TaskType.CAUSAL_LM,\n)\n\r\nmodel = peft.get_peft_model(model,lora_config)  # note: for most peft methods, this line also modifies model in-plac)))","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:27:16.469079Z","iopub.execute_input":"2024-03-03T22:27:16.470002Z","iopub.status.idle":"2024-03-03T22:28:24.565805Z","shell.execute_reply.started":"2024-03-03T22:27:16.469966Z","shell.execute_reply":"2024-03-03T22:28:24.564944Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model.print_trainable_parameters() # Wow so small amount of trainable params","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:28:24.567598Z","iopub.execute_input":"2024-03-03T22:28:24.568030Z","iopub.status.idle":"2024-03-03T22:28:24.581569Z","shell.execute_reply.started":"2024-03-03T22:28:24.567994Z","shell.execute_reply":"2024-03-03T22:28:24.580551Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758499550960753\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:44:04.770783Z","iopub.execute_input":"2024-03-03T21:44:04.771793Z","iopub.status.idle":"2024-03-03T21:44:04.794785Z","shell.execute_reply.started":"2024-03-03T21:44:04.771736Z","shell.execute_reply":"2024-03-03T21:44:04.793703Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): Linear4bit(\n                in_features=4096, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): Linear4bit(\n                in_features=4096, out_features=1024, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): Linear4bit(\n                in_features=4096, out_features=1024, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear4bit(\n                in_features=4096, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(\n                in_features=4096, out_features=14336, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): Linear4bit(\n                in_features=4096, out_features=14336, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): Linear4bit(\n                in_features=14336, out_features=4096, bias=False\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# creating simple prompt formating\ndef format_prompt(sample):\n    messages = []\n    \n    for i in range(len(sample['passage'])):\n        messages.append(f'''You are given question. Answer only \"true\" or \"false\".\n        text: {sample['passage'][i]}\n        question: {sample['question'][i]}\n        answer: {sample['answer'][i]}\n        ''')\n        \n    return messages \n\n# def formatting_prompts_func(example):\n#     output_texts = []\n#     for i in range(len(example['question'])):\n#         text = f\"### Text: {example['passage'][i]}\\n ### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n#         output_texts.append(text)\n#     return output_texts\n\n#  def generate_and_tokenize_prompt(sample):\n#         full_prompt = generate_prompt(sample)\n#         tokenized_message = tokenizer(full_prompt, padding=True, truncation=True, return_tensors=\"pt\")\n#         return tokenized_message\n\ndef tokenize_function(examples):\n    return tokenizer(examples, padding=True, truncation=True)\n\n# data = tokenize_function(format_prompt(df['train'][:1000]))","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:36:31.442224Z","iopub.execute_input":"2024-03-03T22:36:31.443103Z","iopub.status.idle":"2024-03-03T22:36:31.449248Z","shell.execute_reply.started":"2024-03-03T22:36:31.443072Z","shell.execute_reply":"2024-03-03T22:36:31.448244Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"data = format_prompt(df['train'][:500])\ntokenized_data = tokenize_function(data)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:37:50.078093Z","iopub.execute_input":"2024-03-03T22:37:50.078919Z","iopub.status.idle":"2024-03-03T22:37:50.230501Z","shell.execute_reply.started":"2024-03-03T22:37:50.078886Z","shell.execute_reply":"2024-03-03T22:37:50.229719Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"TODO: initialize Trainer and pass train part of our dataset for 2-3 epoches\n\nNote: carefully set max_seq_length and args (that are transformers.TrainingArguments)","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\narguments = TrainingArguments(\n    output_dir = '/kaggle/working/outputs',\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    warmup_steps=10,\n    max_steps=20,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=1,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.05,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=arguments,\n    max_seq_length=min(tokenizer.model_max_length, 1024),\n    formatting_func=formatting_prompts_func,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n    tokenizer=tokenizer,\n    peft_config=lora_config,\n    train_dataset=df['train'][:500]\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T17:23:27.300379Z","iopub.execute_input":"2024-03-03T17:23:27.301381Z","iopub.status.idle":"2024-03-03T17:23:27.753437Z","shell.execute_reply.started":"2024-03-03T17:23:27.301345Z","shell.execute_reply":"2024-03-03T17:23:27.752381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T17:23:41.812652Z","iopub.execute_input":"2024-03-03T17:23:41.813041Z","iopub.status.idle":"2024-03-03T17:31:52.012469Z","shell.execute_reply.started":"2024-03-03T17:23:41.813011Z","shell.execute_reply":"2024-03-03T17:31:52.010829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TODO: save and check your tuned model. Provide scores on our 20 validation examples and save result to csv file","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"trained-model\")","metadata":{},"execution_count":null,"outputs":[]}]}