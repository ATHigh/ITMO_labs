{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9QLe_T6GZUd"
   },
   "source": [
    "# Programming task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYlIf2yHv8hz"
   },
   "source": [
    "**The task should be completed with the current values of the hyperparameters. For verification, below you will see the answers that should be obtained. After all the answers match, you can use the resulting notebook to complete your individual task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDQzNIZXAoFE"
   },
   "source": [
    "Set model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NOMw2ZbOAmOZ"
   },
   "source": [
    "epsilon = 0.1 # Epsilon parameter which is used in epsilon-greedy strategy\n",
    "gamma = 0.8 # Discount coefficient gamma\n",
    "random_seed = 100 #Random seed\n",
    "time_delay = 1 # Time delay when rendering the game process after training (seconds)\n",
    "lr_rate = 0.9 #Learning rate alpha"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQu5IYHX8jId"
   },
   "source": [
    "We import the libraries, create our own 6x6 environment. S denotes the starting point. F - ice is safe (frozen), H - hole, G - goal. The parameter `is_slippery = False` is responsible for slipping. That is, if the agent chose the action to go right, then it will move to the corresponding state. In the general case, due to the “slipping”, one may find itself in a different state. We also copied from the GYM library and slightly modified the function `` generate_random_map ``, in order to generate arbitrary maps based on `` random_seed ``."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "awL7CCCwD6C3",
    "outputId": "1175cc6b-86d5-4bee-f739-eb8124d3a6b9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "!git clone https://github.com/dvolchek/gym_0_18_0.git -q\n",
    "%cd /content/gym_0_18_0\n",
    "!pip install -e. -q\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def generate_random_map(size, p, sd):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    np.random.seed(sd)\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0,0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r,c) in discovered:\n",
    "                discovered.add((r,c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 'G':\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] not in '#H'):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
    "        res[0][0] = 'S'\n",
    "        res[-1][-1] = 'G'\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "# Map generation\n",
    "random_map = generate_random_map(size=6, p=0.8, sd = random_seed) #Create our map\n",
    "env = gym.make(\"FrozenLake-v0\", desc=random_map, is_slippery=False) #Initialize environment\n",
    "print(\"Your map\")\n",
    "env.render() #Render the map"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/gym_0_18_0'\n",
      "/Users/roman14/Desktop/Pycharm/ITMO_labs/BigData_ML_online\n",
      "\u001B[31mERROR: file:///Users/roman14/Desktop/Pycharm/ITMO_labs/BigData_ML_online does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 49\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m res]\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# Map generation\u001B[39;00m\n\u001B[0;32m---> 49\u001B[0m random_map \u001B[38;5;241m=\u001B[39m generate_random_map(size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6\u001B[39m, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m, sd \u001B[38;5;241m=\u001B[39m \u001B[43mrandom_seed\u001B[49m) \u001B[38;5;66;03m#Create our map\u001B[39;00m\n\u001B[1;32m     50\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFrozenLake-v0\u001B[39m\u001B[38;5;124m\"\u001B[39m, desc\u001B[38;5;241m=\u001B[39mrandom_map, is_slippery\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;66;03m#Initialize environment\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour map\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'random_seed' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDCexoEU9a_c"
   },
   "source": [
    "Functions for selecting an action and updating an action value table. The line *** is used to check responses in openedx. Outside of the academic problem, it is better to use the original method of the `environment` class, that is:\n",
    "\n",
    "`action = env.action_space.sample ()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5TbDqn6G_Pt"
   },
   "source": [
    "# Task 1\n",
    "Complete the function `` learn () ``, so that as a result of its call, the value of the current action is updated according to the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CdQBpxaTOK7u"
   },
   "source": [
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0,env.action_space.n) #***\n",
    "    else:\n",
    "        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
    "    return action\n",
    "\n",
    "def learn(state, state2, reward, action, done):\n",
    "    #Q[state, action] = #Your code here\n",
    "    # Оценка ценности текущего состояния и действия\n",
    "    q_value = Q[state, action]\n",
    "\n",
    "    if not done:\n",
    "        # Оценка ценности следующего состояния\n",
    "        next_max = np.max(Q[state2])\n",
    "        # Обновление ценности текущего действия\n",
    "        q_target = reward + gamma * next_max\n",
    "    else:\n",
    "        # Если достигнуто конечное состояние, обновление ценности равно только полученному вознаграждению\n",
    "        q_target = reward\n",
    "\n",
    "    # Обновление ценности текущего действия\n",
    "    Q[state, action] += lr_rate * (q_target - q_value)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7COGeyA_Ist3"
   },
   "source": [
    "# Task 2\n",
    "Complete the following code so that as a result of training the model you could find out the number of wins and the number of the game (`game`), on which the agent won the fifth victory in a row for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0adDl7NvJoQP"
   },
   "source": [
    "Let's explain what the function ```env.step(action)``` returns\n",
    "\n",
    "```state2``` --  next state\n",
    "\n",
    "```reward``` -- reward\n",
    "\n",
    "```done``` -- inidcator of the end of the game. True in case of victory or fall into the hole. False in other cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aq92-dWiOchF"
   },
   "source": [
    "from tqdm import tqdm\n",
    "# Inititalization\n",
    "np.random.seed(random_seed)\n",
    "total_games = 10000\n",
    "max_steps = 100\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#Main cycle\n",
    "for game in tqdm(range(total_games)):\n",
    "    state = env.reset()\n",
    "    t = 0\n",
    "    while t < max_steps:\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        action = choose_action(state)\n",
    "\n",
    "        state2, reward, done, info = env.step(action)\n",
    "\n",
    "        if t == max_steps:\n",
    "          done = True\n",
    "\n",
    "        learn(state, state2, reward, action, done)\n",
    "\n",
    "        state = state2\n",
    "\n",
    "        if done:\n",
    "          break\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFuxsqdRLOS9"
   },
   "source": [
    "Output answers with the given parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xZbJtFnhLa7w"
   },
   "source": [
    "print(\"The number of victories in a series of 10,000 games: \", #Your code here)\n",
    "print(\"Five wins in a row were first won in the game \", #Your code here)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Inititalization\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(\u001B[43mrandom_seed\u001B[49m)\n\u001B[1;32m      6\u001B[0m total_games \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10000\u001B[39m\n\u001B[1;32m      7\u001B[0m max_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'random_seed' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Inititalization\n",
    "np.random.seed(random_seed)\n",
    "total_games = 10000\n",
    "max_steps = 100\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "wins_in_series = 0\n",
    "first_five_wins_game = None\n",
    "total_wins = 0\n",
    "\n",
    "#Main cycle\n",
    "for game in tqdm(range(total_games)):\n",
    "    state = env.reset()\n",
    "    t = 0\n",
    "    while t < max_steps:\n",
    "        t += 1\n",
    "        action = choose_action(state)\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        if t == max_steps:\n",
    "            done = True\n",
    "        learn(state, state2, reward, action, done)\n",
    "        state = state2\n",
    "        if done:\n",
    "            if reward == 1:  # Победа\n",
    "                total_wins += 1\n",
    "                wins_in_series += 1\n",
    "                if wins_in_series == 5 and first_five_wins_game is None:\n",
    "                    first_five_wins_game = game\n",
    "            else:\n",
    "                wins_in_series = 0  # Сбрасываем счетчик серии побед\n",
    "            break\n",
    "\n",
    "print(\"Количество побед в серии из 10 000 игр:\", total_wins)\n",
    "print(\"Пять побед подряд впервые было одержано в игре\", first_five_wins_game+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSXdSiG2WI71"
   },
   "source": [
    "The following results should be obtained.\n",
    "\n",
    "\n",
    "*  The number of victories in a series of 10,000 games:  7914\n",
    "*  Five wins in a row were first won in the game  885\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nazZaAbwQGBt"
   },
   "source": [
    "Let's perform one game to track the actions of the agent. At the same time, we will consider the model fully trained, that is, actions are selected according to the greedy strategy, the values of the actions in the table are not updated."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5ysllZjEQXLa"
   },
   "source": [
    "import time\n",
    "#Greedy action selection\n",
    "def choose_action_one_game(state):\n",
    "    action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
    "    return action\n",
    "\n",
    "states=[]#Array to save agent states during the game\n",
    "t = 0\n",
    "state = env.reset()\n",
    "wn = 0\n",
    "while(t<100):\n",
    "  env.render()\n",
    "  time.sleep(time_delay)\n",
    "  clear_output(wait=True)\n",
    "  action = choose_action_one_game(state)\n",
    "  state2, reward, done, info = env.step(action)\n",
    "  states.append(state)\n",
    "  state = state2\n",
    "  t += 1\n",
    "  if done and reward == 1:\n",
    "    wn=1\n",
    "  if done:\n",
    "    break\n",
    "if wn == 1:\n",
    "  print(\"!!!WIN!!!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x696NulpReFI"
   },
   "source": [
    "Route map"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UKMCMdpOTcXy"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_maze_pic(maze):\n",
    "  maze_pic=[]\n",
    "  for i in range(len(maze)):\n",
    "    row = []\n",
    "    for j in range(len(maze[i])):\n",
    "      if maze[i][j] == 'S':\n",
    "        row.append(0)\n",
    "      if maze[i][j] == 'F':\n",
    "        row.append(0)\n",
    "      if maze[i][j] == 'H':\n",
    "        row.append(1)\n",
    "      if maze[i][j] == 'G':\n",
    "        row.append(0)\n",
    "    maze_pic.append(row)\n",
    "  maze_pic = np.array(maze_pic)\n",
    "  return maze_pic\n",
    "\n",
    "\n",
    "#Make maze fit to plot\n",
    "maze_pic = make_maze_pic(random_map)\n",
    "nrows, ncols = maze_pic.shape\n",
    "\n",
    "#Arrays of picture elements\n",
    "rw = np.remainder(states,nrows)\n",
    "cl = np.floor_divide(states,nrows)\n",
    "if wn == 1:\n",
    "  rw = np.append(rw, [nrows-1])\n",
    "  cl = np.append(cl,[ncols-1])\n",
    "\n",
    "#Picture plotting\n",
    "fig, ax1 = plt.subplots(1, 1, tight_layout=True)\n",
    "ax1.clear()\n",
    "ax1.set_xticks(np.arange(0.5, nrows, step=1))\n",
    "ax1.set_xticklabels([])\n",
    "ax1.set_yticks(np.arange(0.5, ncols, step=1))\n",
    "ax1.set_yticklabels([])\n",
    "ax1.grid(True)\n",
    "ax1.plot([0],[0], \"gs\", markersize=40)  # start is a big green square\n",
    "ax1.text(0, 0.2,\"Start\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Start text\n",
    "ax1.plot([nrows-1],[ncols-1], \"rs\", markersize=40)  # exit is a big red square\n",
    "ax1.text(nrows-1, ncols-1+0.2,\"Finish\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Exit text\n",
    "ax1.plot(rw,cl, ls = '-', color = 'blue') #Blue lines path\n",
    "ax1.plot(rw,cl, \"bo\")  # Blue dots visited cells\n",
    "ax1.imshow(maze_pic, cmap=\"binary\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}